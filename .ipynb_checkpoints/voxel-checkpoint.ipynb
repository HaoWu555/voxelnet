{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import argparse\n",
    "import os\n",
    "from config import cfg\n",
    "from easydict import EasyDict\n",
    "from utils.kitti_loader import iterate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser=EasyDict()\n",
    "parser.i = 1\n",
    "parser.tag = 'Test1'\n",
    "parser.single_batch_size = 2\n",
    "parser.lr =0.001\n",
    "parser.al =1\n",
    "parser.output_path = './prediction'\n",
    "parser.v=False\n",
    "\n",
    "dataset_dir = cfg.DATA_DIR\n",
    "train_dir = os.path.join(cfg.DATA_DIR, 'training')\n",
    "val_dir = os.path.join(cfg.DATA_DIR, 'validation')\n",
    "log_dir = os.path.join('./log', parser.tag)\n",
    "save_model_dir = os.path.join('./save_model', parser.tag)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(save_model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#voxel feature encoding\n",
    "class VFELayer(object):\n",
    "    def __init__(self,out_channels,name):\n",
    "        super(VFELayer,self).__init__()\n",
    "        self.units=int(out_channels/2)\n",
    "        self.dense=tf.keras.layers.Dense(self.units,activation=tf.nn.relu,name='dense')\n",
    "        self.batch_norm=tf.keras.layers.BatchNormalization(name=\"batch_norm\",fused=True)\n",
    "    def __call__(self,inputs,mask,training):\n",
    "        dense = self.dense(inputs)\n",
    "        #[K,T,units]\n",
    "        pointwise = self.batch_norm(dense) \n",
    "        #[K,1,units]\n",
    "        aggregated = tf.reduce_max(input_tensor=pointwise,axis=1,keepdims=True)\n",
    "        #[K,T,units]\n",
    "        repeated = tf.tile(aggregated,[1,cfg.VOXEL_POINT_COUNT,1])\n",
    "        #[K,T,2*units] \n",
    "        concatenated = tf.concat([pointwise,aggregated],axis=2)\n",
    "        \n",
    "        mask = tf,tile(mask,[1,1,2*self.units])\n",
    "        \n",
    "        concatenated = tf.multiply(concatenated,tf.cast(mask,tf.float32))\n",
    "        \n",
    "        return concatenated\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureNet(object):\n",
    "    def __init__(self,training,batch_size,name=''):\n",
    "        super(FeatureNet,self).__init__()\n",
    "        self.training=training\n",
    "        \n",
    "        #scalar\n",
    "        self.batch_size=batch_size\n",
    "        #self.feature\n",
    "        #self.number\n",
    "        #self.coordinate\n",
    "        self.vfe1=VFELayer(32,'VFE-1')\n",
    "        self.vfe2=VFELayer(128,'VFE-2')\n",
    "        \n",
    "        # boolean mask [K,T,2*units]\n",
    "        # elimate the empty voxel\n",
    "        mask = tf.not_equal(tf.reduce_max(input_tensor=self.feature,axis=2,keepdims=True),0)\n",
    "        x = self.vfe1(self.feature,mask,self.training)\n",
    "        x = self.vfe2(x,mask,self.training)\n",
    "        # [K,128]\n",
    "        voxelwise = tf.reduce_max(input_tensor=x,axis=1)\n",
    "        # car: [N * 10 * 400 * 352 * 128]\n",
    "        # pedestrian/cyclist: [N * 10 * 200 * 240 * 128]\n",
    "        self.outputs = tf.scatter_nd(\n",
    "            self.coordinate, voxelwise, [self.batch_size, 10, cfg.INPUT_HEIGHT, cfg.INPUT_WIDTH, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.VFELayer at 0x7f0e1818d898>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=FeatureNet(training=True,batch_size=1)\n",
    "A.vfelayer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPN3D(object):\n",
    "    def __init__(self,cls=\"Car\",single_batch_size=1,learning_rate=0.001,max_gradient_norm=5.0,\n",
    "                 alpha=1.5,beta=1,avail_gpus=['0']):\n",
    "        self.cls = cls\n",
    "        self.single_batch_size = single_batch_size\n",
    "        self.learning_rate = tf.Variable(float(learning_rate), trainable=False, dtype=tf.float32)\n",
    "        self.global_step = tf.Variable(1, trainable=False)\n",
    "        self.epoch = tf.Variable(0, trainable=False)\n",
    "        self.epoch_add_op = self.epoch.assign(self.epoch + 1)\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.avail_gpus = avail_gpus\n",
    "         \n",
    "        boundaries = [80, 120]\n",
    "        values = [ self.learning_rate, self.learning_rate * 0.1, self.learning_rate * 0.01 ]\n",
    "        self.lr = tf.compat.v1.train.piecewise_constant(self.epoch, boundaries, values)\n",
    "\n",
    "        #build graph\n",
    "        self.is_train=True\n",
    "        \n",
    "        self.vox_feature = []\n",
    "        self.vox_number = []\n",
    "        self.vox_coordinate = []\n",
    "        self.targets = []\n",
    "        self.pos_equal_one = []\n",
    "        self.pos_equal_one_sum = []\n",
    "        self.pos_equal_one_for_reg = []\n",
    "        self.neg_equal_one = []\n",
    "        self.neg_equal_one_sum = []\n",
    "        \n",
    "        self.delta_output = []\n",
    "        self.prob_output = []\n",
    "        #self.opt = tf.compat.v1.train.AdamOptimizer(lr)\n",
    "        self.gradient_norm = []\n",
    "        self.tower_grads = []\n",
    "        \n",
    "        def __call__(vox_coordinate,)\n",
    "            for idx, dev in enumerate(self.avail_gpus):\n",
    "                if idx==2:\n",
    "                    break\n",
    "                with tf.device('/gpu:{}'.format(dev)):\n",
    "                    feature=FeatureNet(training=self.is_train, batch_size=self.single_batch_size)\n",
    "                    #rpn = MiddleAndRPN(input=feature.outputs, alpha=self.alpha, beta=self.beta, training=self.is_train)\n",
    "                    # input\n",
    "                    self.vox_feature.append(feature.feature)\n",
    "                    self.vox_number.append(feature.number)\n",
    "                    self.vox_coordinate.append(feature.coordinate)\n",
    "                    #self.targets.append(rpn.targets)\n",
    "                    #self.pos_equal_one.append(rpn.pos_equal_one)\n",
    "                    #self.pos_equal_one_sum.append(rpn.pos_equal_one_sum)\n",
    "                    #self.pos_equal_one_for_reg.append(rpn.pos_equal_one_for_reg)\n",
    "                    #self.neg_equal_one.append(rpn.neg_equal_one)\n",
    "                    #self.neg_equal_one_sum.append(rpn.neg_equal_one_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in iterate_data(train_dir):\n",
    "    batch[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FeatureNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-e5d3a0f27d84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRPN3D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-84168be16438>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cls, single_batch_size, learning_rate, max_gradient_norm, alpha, beta, avail_gpus)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavail_gpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/gpu:{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mfeature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFeatureNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                 \u001b[0mrpn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMiddleAndRPN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;31m# input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FeatureNet' is not defined"
     ]
    }
   ],
   "source": [
    "model=RPN3D()\n",
    "model.lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "# batch: (tag,labels,vox_feature,vox_number,vox_coordinate,rgb,raw_lidar)\n",
    "for batch in iterate_data(train_dir):\n",
    "    i+=1\n",
    "    if i == 2:\n",
    "        break\n",
    "    else:\n",
    "        pass\n",
    "        #print(\"batch:\",batch[2])\n",
    "        # build model\n",
    "        #input data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
